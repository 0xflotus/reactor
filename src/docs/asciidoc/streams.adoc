"Nope, you shall not use Future.get(), ever."
-- Stephane Maldini, with a Banking Sector Customer

.Head first with a Java 8 example of some Stream work
[source,java]
----
import static reactor.Environment.*;
import reactor.rx.Streams;
import reactor.rx.BiStreams;

//...

Environment.initialize()

//find the top 10 words used in a list of String
Streams.from(aListOfString)
  .dispatchOn(sharedDispatcher())
  .flatMap(sentence ->
    Streams
      .from(sentence.split(" "))
      .dispatchOn(cachedDispatcher())
      .filter(word -> !word.trim().isEmpty())
      .observe(word -> doSomething(word))
  )
  .map(word -> Tuple.of(word, 1))
  .window(1, TimeUnit.SECONDS)
  .flatMap(words ->
    BiStreams.reduceByKey(words, (prev, next) -> prev + next)
      .sort((wordWithCountA, wordWithCountB) -> -wordWithCountA.t2.compareTo(wordWithCountB.t2))
      .take(10)
      .finallyDo(event -> LOG.info("---- window complete! ----"))
  )
  .consume(
    wordWithCount -> LOG.info(wordWithCount.t1 + ": " + wordWithCount.t2),
    error -> LOG.error("", error)
  );
----

[[streams]]
== Coordinating tasks with Streams and Promises

.How Doge can use Reactor-Stream
image::images/streams-overview.png[Stream Overview, width=650, align="center", link="images/streams-overview.png"]


*Reactor Streams* has the following artefacts:

****
* Static *Factories*, the one-stop-shops to create the related components
** `Streams` for `Stream` creation from well-defined data sources (iterable, nothing, future, Publisher...)
** `BiStreams` for key-value `Stream<Tuple2>` processing (reduceByKey...)
** `IOStreams` for <<streams.adoc#streams-persistent, Persisting>> and <<core-codec#core-codecs, Decoding>> `Streams`
** `Promises` for single-data-only `Promise`
* `Stream` and its direct implementations
** Contains *reactive extensions* and other composition API
** The extensions are usually created with the factories (`Streams.from(Iterable)` -> IterableStream, `Streams.from(Future)` -> FutureStream...)
* `Promise` with a specific https://promisesaplus.com[A+ flavored] API
** Can be transformed back to Stream with `Promise.stream()`
* `Action` and its direct implementations of every operation provided by the `Stream` following the *Reactive Streams Processor* specification
** They are not created directly in general, but with the Stream API (`Stream.map()` -> MapAction, `Stream.filter()` -> FilterAction...)
* `Broadcaster`, the publish/subscribe interfaces for dynamic data dispatch
** Unlike <<core-processor#core-processor,Core Processors>>, they will usually not bother buffering data if there is no subscriber attached
** However the `BehaviorBroadcaster` can replay the latest signal to the new Subscribers
****

[NOTE]
Do not confuse `reactor.rx.Stream` with the new JDK 8 `java.util.stream.Stream`. The latter does not offer a Reactive Streams based API nor Reactive Extensions.
However the API is quite complete around primitive types and Collections, in fact it's quite interesting for JDK 8 enabled applications to mix them both.

[[streams-basics]]
== Streams Basics
Reactor offers `Stream` or `Promise` based on the Reactive Streams standard to compose statically typed data pipelines.

It is an incredibly useful and flexible component. It's flexible enough to be used to just compose asynchronous actions together like RxJava's `Observable`. But it's powerful enough it can function as an asynchronous work queue that forks and joins arbitrary compositions or other Reactive Streams components coming from one of the other implementors of the standard.footnoteref:[reactive-streams-implementors, including http://akka.io[Akka Streams], http://ratpack.io[Ratpack], and https://github.com/ReactiveX/RxJava[RxJava]]

.There are basically two rough categories of streams
****
* A *hot* `Stream` is unbounded and capable of accepting input data like a sink.
** Think *UI events* such as _mouse clicks_ or realtime *feeds* such as _sensors_, _trade positions_ or _Twitter_.
** Adapted backpressure strategies mixed with the Reactive Streams protocol will apply
* A *cold* `Stream` is bounded and generally created from a fixed collection of data like a `List` or other `Iterable`.
** Think *Cursored Read* such as _IO reads_, _database queries_,
** Automatic Reactive Streams backpressure will apply
****

[NOTE]
====
As seen <<core.adoc#core-dispatchers, previously>>, Reactor uses an `Environment` to keep sets of `Dispatcher` instances around for shared use in a given JVM (and classloader). An `Environment` instance can be created and passed around in an application to avoid classloading segregation issues or the static helpers can be used. Throughout the examples on this site, we'll use the static helpers and encourage you to do likewise. To do that, you'll need to initialize the static `Environment` somewhere in your application.

[source,java]
----
static {
  Environment.initialize();
}
----
====

== Creating Streams and Promises

This is where you start if you are the owner of the data-source and want to just make it Reactive with direct access to various _Reactive Extensions_ and _Reactive Streams_ capacities.

Sometimes it's also a case for expanding an existing *Reactive Stream Publisher* with `Stream` API and we fortunately offer one-shot static API to proceed to the conversion.

Extending existing Reactor `Stream` like we do with `IterableStream`, `SingleValueStream` etc is also an incentive option to create a `Publisher` ready source (Stream implements it) injected with Reactor API.

[IMPORTANT]
====
Streams and Promises are relatively inexpensive, our microbenchmark suite succeeds into creating more than 150M/s on commodity hardware.
Most of the Streams stick to the *Share-Nothing* pattern, only creating new immutable objects when required.

Every operation will return a new instance:
[source, java]
----
Streams<A> stream = Streams.just(a);
Streams<B> transformedStream = stream.map(transformationToB);

Assert.isTrue(transformationStream != stream);
stream.subscribe(subscriber1); //subscriber1 will see the data A unaltered
transformedStream.subscribe(subscriber2); //subscriber2 will see the data B after transformation from A.

//Note theat these two subscribers will materialize independant stream pipelines, a process we also call lifting
----
====

=== From Cold Data Sources

You can create a `Stream` from a variety of sources, including an `Iterable` of known values, a single value to use as the basis for a flow of tasks, or even from blocking structures such as `Future` of `Supplier`.

.Streams.just()
[source,java]
----
Stream<String> st = Streams.just("Hello ", "World", "!"); // <1>

st.dispatchOn(Environment.cachedDispatcher()) // <2>
  .map(String::toUpperCase) // <3>
  .consume(s -> System.out.printf("%s greeting = %s%n", Thread.currentThread(), s)); // <4>
----
<1> Create a `Stream` from a known value but do not assign a default `Dispatcher`.
<2> `.dispatchOn(Dispatcher)` tells the `Stream` which thread to execute tasks on. Use this to move execution from one thread to another.
<3> Transform the input using a commonly-found convention: the map() method.
<4> Produce demand on the pipeline, which means "start processing now". It's an optimize shortcut for `subscribe(Subscriber)` where the Subscriber just requests Long.MAX_VALUE by default.

[IMPORTANT]
Cold Data Sources will be replayed from start for every fresh Subscriber passed to `Stream.subscribe(Subscriber)`, and therefore duplicate consuming is possible.

.Creating pre-determined Streams and Promises
[cols="2,1,6", options="header"]
|===

|Factory method
|Data Type
|Role

|Streams.<T>empty()
|T
|Only emit `onComplete()` once *requested by its Subscriber*.

|Streams.<T>never()
|T
|Never emit anything. Useful for keep-alive behaviors.

|Streams.<T, Throwable>fail(*Throwable*)
|T
|Only emit `onError(Throwable)`.

|Streams.from(*Future<T>*)
|T
|Block the `Subscription.request(long)` on the passed `Future.get()` that might emit `onNext(T)` and `onComplete()` otherwise `onError(Throwable)` for any exception.

|Streams.from(*T[]*)
|T
|Emit N `onNext(T)` elements everytime `Subscription.request(N)` is invoked. If N == Long.MAX_VALUE, emit everything. Once all the array has been read, emit `onComplete()`.

|Streams.from(*Iterable<T>*)
|T
|Emit N `onNext(T)` elements everytime `Subscription.request(N)` is invoked. If N == Long.MAX_VALUE, emit everything. Once all the array has been read, emit `onComplete()`.

|Streams.range(*long*, _long_)
|Long
|Emit a sequence of N `onNext(Long)` everytime `Subscription.request(N)` is invoked. If N == Long.MAX_VALUE, emit everything. Once the inclusive upper bound been read, emit `onComplete()`.

|Streams.just(T, _T, T, T, T, T, T, T_)
|T
|An optimization over `Streams.from(Iterable)` that just behaves similarly. Also useful to emit Iterable, Array or Future without colliding with the Streams.from() signatures.

|Streams.generate(*Supplier<T>*)
|T
|Emit `onNext(T)` from the producing `Supplier.get()` factory everytime `Subscription.request(N)` is called. The demand N is ignored as only one data is emitted. When a null value is returned, emit `onComplete()`.

|Promises.syncTask(Supplier<T>), Promises.task(, Supplier<T>)
|T
|Emit a single `onNext(T)` and `onComplete()` from the producing `Supplier.get()` on the first `Subscription.request(N)` received. The demand N is ignored.

|Promises.success(*T*)
|T
|Emit `onNext(T)` and `onComplete()` whenever a `Subscriber` is provided to `Promise.subscribe(Subscriber)`.

|Promises.<T>error(*Throwable*)
|T
|Emit `onError(Throwable)` whenever a `Subscriber` is subscribed is provided to `Promise.subscribe(Subscriber)`.

|===

[[streams-reactivestreams]]
=== From Existing Reactive Publishers

Existing Reactive Streams `Publishers` can very well be from other implementations, including the user ones, or from Reactor itself.

The use cases incude:
****
* <<streams.adoc#streams-combine, Combinatory API>> to coordinate various data sources.
* Lazy resource access, reading a Data Source on subscribe or on request, e.g. _Remote HTTP calls_.
* Data-oriented operations such as Key/Value `Tuples Streams`, `Persistent Streams` or Decoding.
* Plain Publisher decoration with `Stream API`
****

.Streams.concat() and Streams.wrap() in action
[source,java]
----
Processor<String,String> processor = RingBufferProcessor.create();

Stream<String> st1 = Streams.just("Hello "); // <1>
Stream<String> st2 = Streams.just("World "); // <1>
Stream<String> st3 = Streams.wrap(processor); // <2>

Streams.concat(st1, st2, st3) // <3>
  .reduce( (prev, next) -> prev + next ) // <4>
  .consume(s -> System.out.printf("%s greeting = %s%n", Thread.currentThread(), s)); // <5>

processor.onNext("!");
processor.onComplete();
----
<1> Create a `Stream` from a known value.
<2> Decorate the core processor with `Stream` API. Note that `Streams.concat()` would have accepted the processor directly as a valid Publisher argument.
<3> Concat the 3 upstream sources (all st1, then all st2, then all st3).
<4> Accumulate the input 2 by 2 and emit the result on upstream completion, after the last complete from st3.
<5> Produce demand on the pipeline, which means "start processing now".

.Creating from available Reactive Streams Publishers
[cols="2,1,6", options="header"]
|===

|Factory method
|Data Type
|Role

|Streams.create(*Publisher<T>*)
|T
|Only subscribe to the passed `Publisher` when the first `Subscription.request(N)` hits the returned `Stream`.
Therefore it supports malformed Publishers that do not invoke `Subscriber.onSubscribe(Subscription)` as required per specification.

|Streams.wrap(*Publisher<T>*)
|T
|A simple delegating `Stream` to the passed `Publisher.subscribe(Subscriber<T>)` argument. Only supports _well formed_ Publishers correctly using the Reactive Streams protocol *(onSubscribe > onNext\* > (onError \| onComplete)*).

|Streams.defer(*Supplier<Publisher<T>>*)
|T
|A lazy Publisher access using the level of indirection provided by `Supplier.get()` everytime `Stream.subscribe(Subscriber)` is called.

|Streams.switchOnNext(*Publisher<Publisher<T>>*)
|T
|A Stream alterning in FIFO order between emitted `onNext(Publisher<T>)` from the passed Publisher. The signals will result in downstream Subscriber<T> receiving the next Publisher sequence of `onNext(T)`.
It might interrupt a current upstream emission when the `onNext(Publisher<T>)` signal is received.

|Streams.concat(*Publisher<T>*, _Publisher<T>*_),
Streams.concat(*Publisher<Publisher<T>>*)
|T
|If a Publisher<T> is already emitting, wait for it to `onComplete()` before draining the next pending Publisher<T>. As the name suggests its useful to http://rxmarbles.com/#concat[concat various datasources] and keep ordering right.

|Streams.merge(*Publisher<T>, Publisher<T>*, Publisher<T>*),
Streams.merge(*Publisher<Publisher<T>>*)
|T
|http://rxmarbles.com/#merge[Accept multiple sources] and *interleave* their respective sequence. Order won't be preserved like with `concat`. Demand from a Subscriber will be splitted between various source with a minimum of 1 by upstream to make sure everyone has a chance to send something.

|Streams.combineLatest(*Publisher<T1>, Publisher<T2>*, _Publisher<T3-N>\*_, *Function<Tuple2-N, C>*)
|C
|http://rxmarbles.com/#combineLatest[Combine most recent emitted elements] from the passed sources using the given `Function` aggregating function.

|Streams.combineLatest(*Publisher<T1>, Publisher<T2>,* _Publisher<T3-N>\*_, *Function<Tuple2-N, C>*)
|C
|http://rxmarbles.com/#zip[Combine most recent emitted elements once], everytime every source has emitted a signal, apply the given `Function` and clear the temporary aggregate. Effectively it's a flexible _join_ mechanism for multiple different type of sources.

|Streams.join(*Publisher<T>, Publisher<T>*, _Publisher<T>*>_)
|List<T>
|A shortcut for zip that only aggregate each complete aggregate in a List matching the order of the passed argument sources.

|Streams.await(*Publisher<>*, _long, unit, boolean_)
|void
|Block the calling thread until the passed `Publisher` `onComplete` the waiting internal `Subscriber`. Optional arguments to tune the timeout and the need to request data as well can be passed. It will throw an exception if the final state is `onError` instead.

|IOStreams.<K,V>persistentMap(*String*, _deleteOnExit_)
|V
|<<streams.adoc#streams-persistent, A simple shortcut over ChronicleStream constructors>>, a disk-based log appender/tailer. The name argument must match an existing persistent queue under /tmp/persistent-queue\[name\].

|IOStreams.<K,V>persistentMapReader(*String*)
|V
|<<streams.adoc#streams-persistent, A simple shortcut over ChronicleReaderStream constructors>>, a disk-based log tailer. The name argument must match an existing persistent queue under /tmp/persistent-queue\[name\].

|IOStreams.decode(*Codec<SRC, IN, ?>, Publisher<SRC>*)
|IN
|Use <<core.adoc#core-codecs, Codec decoder>> to decode the passed source data type into *IN* type.

|BiStreams.reduceByKey(*Publisher<Tuple2<KEY,VALUE>>*, _Map<KEY,VALUE>, Publisher<MapStream.Signal<KEY, VALUE>>_, *BiFunction<VALUE, VALUE, VALUE>*)
|Tuple2<KEY,VALUE>
|A key-value operation that accumulates computed results for each 2 sequential `onNext(VALUE)` passed to the `BiFunction` argument. The result will be released `onComplete()` only. The options allow to use an existing map store and listen for its events.

|BiStreams.scanByKey(*Publisher<Tuple2<KEY,VALUE>>*, _Map<KEY,VALUE>, Publisher<MapStream.Signal<KEY, VALUE>>_, *BiFunction<VALUE, VALUE, VALUE>*)
|Tuple2<KEY,VALUE>
|A key-value operation that accumulates computed results for each 2 sequential `onNext(VALUE)` passed to the `BiFunction` argument. The result will be released every time just after it has been stored.  The options allow to use an existing map store and listen for its events.

|Promises.when(*Promise<T1>, Promise<T2>*, _Promise<T3-N>*_)
|TupleN<T1,T2,*?>
|Join all unique results from `Promises` and provide for the new `Promise` with the aggregated `Tuple`.

|Promises.any(*Promise<T>, Promise<T>*, _Promise<T>*_)
|T
|Pick the first signal available among the passed promises and `onNext(T)` the returned `Promise` with this result.

|Promises.multiWhen(*Promise<T>, Promise<T>*, _Promise<T>*_)
|List<T>
|Join all unique results from `Promises` and provide for the new `Promise` with the aggregated `List`. The difference with the `when` alternative is that the type of promises must match.

|===

=== From Custom Reactive Publishers

Over time, the Reactor user will become more familiar with the *Reactive Streams*. That's the perfect moment to start custom reactive data-sources!
Usually the implementor would have to respect the specification and verify his work with the *reactive-streams-tck* dependency.
Respecting the contract requires a *Subscription* and a call to *onSubscribe* + a *request(long)* before sending any data.

However Reactor allows some flexibility to only deal with the message passing part and will automatically provide the buffering *Subscription* transparently,
the difference is demonstrated in the code sample below.

.Streams.create and Streams.defer in action
[source,java]
----
final Streams<String> stream1 = Streams.create(sub -> {

  sub.onSubscribe(new Subscription(){ // <1>
    void request(long demand){
      if(demand == 2L){
        sub.onNext(1);
        sub.onNext(2);
        sub.onComplete();
      }
    }

    void cancel(){
      System.out.println("Cancelled!");
    }
  });
});

final Streams<String> stream2 = Streams.create(sub -> {
  sub.onNext(3); // <2>
  sub.onNext(4);
  sub.onComplete();
});

final AtomicInteger counterSubscribe = new AtomicInteger();

Streams<String> deferred = Streams.defer( () -> {
  if(counterSubscriber.incrementAndGet() == 1){ // <3>
    return stream1;
  }else{
    return stream2;
  }
});

deferred
  .consume(s -> System.out.printf("%s greeting = %s%n", Thread.currentThread(), s)); // <4>
----
<1> Create a `Stream` from a custom valid `Publisher` which first calls `onSubscribe(Subscription)`.
<2> Create a `Stream` from a custom malformed `Publisher which skips `onSubscribe(Subscription)` and immediately calls `onNext(T)`.
<3> Create a `DeferredStream` that will alternate source Publisher<T> on each `Stream.subscribe` call, evaluating the total number of Subscribers,

Where to go from here ? Well there are tons of use cases that can benefit from a custom Publisher:

****
* Reactive Facade to convert any IO call with a matching demand and compose: HTTP calls (read N times), SQL queries (select max N), File reads (read N lines)...
* Async Facade to convert any hot data callback into a composable API: AMQP Consumer, Spring MessageChannel endpoint...
****

Reactor offers some done-deal facilities to avoid boilerplate checking you would have to do without extending exsiting Stream or `PushSubscription`

* Extending `PushSubscription` instead of implementing directly `Subscription` to benefit from terminal state (PushSubscription.isComplete())
* Extending `Stream` instead of implementing directly `Publisher` to benefit from composition API

A good place to start feeling the reactive streams way is to simply look at a more ellaborated back-pressure ready <<recipes.adoc#recipe-filestream, File Stream>>.

=== From Hot Data Sources

If you are dealing with an unbounded stream of data items like what would be common with a web application that accepts user input via a REST interface, you probably want to use the "hot" variety of `Stream` in Reactor, which we call a link:/docs/api/index.html?reactor/rx/stream/Broadcaster.html[Broadcaster].

To use it, you simply declare a pipeline of composable, functional tasks on the `Broadcaster` and later call link:/docs/api/reactor/rx/stream/Broadcaster.html#onNext-O-[`Broadcaster.onNext(T)`] to publish values into the pipeline.

[NOTE]
`Broadcaster` are valid `Processor` and `Consumer`. It's possible to `onSubscribe` a Broadcaster as it's also possible to use it as a `Consumer` delegating `Consumer.accept(T)` to `Broadcaster.onNext(T)`.

.Broadcaster.create()
[source,java]
----
Broadcaster<String> sink = Broadcaster.create(Environment.get()); // <1>

sink.map(String::toUpperCase) // <2>
    .consume(s -> System.out.printf("%s greeting = %s%n", Thread.currentThread(), s)); // <3>

sink.onNext("Hello World!"); // <4>
----
<1> Create a `Broadcaster` using the default, shared `RingBufferDispatcher` as the `Dispatcher`.
<2> Transform the input using a commonly-found convention: the map() method.
<3> `.consume()` is a "terminal" operation, which means it produces demand in Reactive Streams parlance.
<4> Publish a value into the pipeline, which will cause the tasks to be invoked.

[IMPORTANT]
Hot Data Sources will never be replayed. Subscribers will only see data from the moment T they have been passed to `Stream.subscribe(Subscriber)`.
An exception applies for `BehaviorBroadcaster` (last emitted element is replayed); `Streams.timer()` and `Streams.period()` will also maintain a unique timed cursors but will still ignore backpressure.

[IMPORTANT]
Subscribers will see new data N flowing through a Broadcaster every T+I^N^ *only* after they have subscribed at time T.

.Creating undetermined Streams
[cols="4*", options="header, autowidth"]
|===

|Factory
|Input
|Output
|Role

|Streams.timer(*delay*, _unit, timer_)
|N/A
|Long
|Start a Timer on `Stream.subscribe(Subscriber)` call and emit a single `onNext(0L)` then `onComplete()` once the delay is elapsed. Be sure to pass the optional argument `Timer` if there is no current active `Environment`.
`Subscription.request(long)` will be ignored as no backpressure can automatically apply to a scheduled emission.

|Streams.period(*period*, _unit, timer_)
|N/A
|Long
|Start a Timer on `Stream.subscribe(Subscriber)` call and every period of time emit `onNext(N)` where N is an incremented counter starting from 0. Be sure to pass the optional argument `Timer` if there is no current active `Environment`.
`Subscription.request(long)` will be ignored as no backpressure can automatically apply to a scheduled emission.

|Streams.<T>switchOnNext()
|Publisher<T>
|T
|An `Action` which for the record is also a `Processor`. The `onNext(Publisher<T>)` signals will result in downstream Subscriber<T> receiving the next Publisher sequence of `onNext(T)`.
 It might interrupt a current upstream emission when the `onNext(Publisher<T>)` signal is received.

|Broadcaster.<T>create(_Environment, Dispatcher_)
|T
|T
|Create a _hot_ bridge between any context allowed to call `onSubscribe`, `onNext`, `onComplete` or `onError` and a composable sequence of these signals under a `Stream`. If no subscriber are actively registered, next signals might trigger a `CancelException`. The optional `Dispatcher` and `Environment` arguments define how to run each signal. Finally, a Broadcaster can be subscribed anytime to a `Publisher`, like a `Stream`.

|SerializedBroadcaster.create(_Environment, Dispatcher_)
|T
|T
|Similar to `Broadcaster.create()` but adds support for concurrent `onNext` from parallel contexts possibly calling the same broadcaster `onXXX` methods.

|BehaviorBroadcaster.create(_Environment, Dispatcher_)
|T
|T
|Simlar to `Broadcaster.create()` but always replays the last data signal if any *and* the last terminal signal (`onComplete()`, `onError(Throwable)`) to the new Subscribers.

|BehaviorBroadcaster.first(*T*, _Environment, Dispatcher_)
|T
|T
|Similar to BehaviorBroadcaster but starts with a default value T.


|Streams.wrap(*Processor<I, O>*)
|I
|O
|A simple delegating `Stream` to the passed `Publisher.subscribe(Subscriber<O>)` argument. Only supports _well formed_ Publishers correctly using the Reactive Streams protocol *(onSubscribe > onNext\* > (onError \| onComplete)*).


|Promises.<T>prepare(*Environment, Dispatcher*), Promises.ready()
|T
|T
|Prepare a `Promise` ready to be called *once only* by any external context through `onNext`. Since it's a stateful container holding the result of the fulfilled promise, new subscribers will immediately run on the current thread.

|===


[TIP]
====
For Asynchronous broadcasting, always consider a <<core-processor.adoc#core-processor,Core Processor>> alternative to a `Broadcaster`:
****
* A Broadcaster will trigger a http://projectreactor.io/docs/api/reactor/core/processor/CancelException.html[CancelException] if there is no subscribers. A Core `RingBuffer*Processor` will always deliver the missed data to the first subscriber.
* Default Synchronous dispatcher and some other that can be assigned to a broadcaster might not support concurrent `onNext`. Use `RingBuffer*Processor.share()` an alternative thread-safe concurrent `onNext`.
* RingBuffer*Processor supports replaying an event cancelled in-flight by a downstream subscriber if it's still running under the processor thread. A Broadcaster won't support replaying.
* RingBuffer*Processor are faster than their alternative Broadcaster with a RingBufferDispatcher
* RingBufferWorkProcessor supports scaling-up along with the number of attached subscribers.
****
====

[[wireup]]
=== Wiring a Stream up

Streams operations minor a few exceptions (terminal actions and `broadcast()`) will never directly subscribe. Instead they will lazily prepare for subscribe.
This is usually called *lift* in Functional programming.

That basically means the Reactor Stream user will explicitely call `Stream.subscribe(Subscriber)` or alternative *terminal* actions such as `Stream.consume(Consumer)` to materialize all the registered operations.
Before that, `Actions` don't really exist, we use `Stream.lift(Supplier)` to defer the creation of these Actions until `Stream.subscribe(Subscriber)` is explicitely called.

Once everything is wired, each action maintains an upstream `Subscription` and a downstream `Subscription` and the Reactive Streams contract applies all along the pipeline.

[IMPORTANT]
Usually the terminal actions return a `Control` object instead of `Stream`.
This is an interface with user-code to request or cancel a pipeline without being inside a `Subscriber` context or implementing the full `Subscriber` contract.

.Wiring up 2 pipelines
[source, java]
----
import static reactor.Environment.*;
import reactor.rx.Streams;
import reactor.rx.Stream;
//...

Stream<String> stream = Streams.just("a","b","c","d","e","f","g","h");

//prepare two unique pipelines
Streams<Long> actionChain1 = stream.map(String::toUpperCase).filter(w -> w.equals("c"));
Streams<Long> actionChain2 = stream.dispatchOn(sharedDispatcher()).take(5).count();

actionChain1.consume(System.out::println); //start chain1
Control c = actionChain2.consume(System.out::println); //start chain2
//...
c.cancel(); //force this consumer to stop receiving data
----

.After Wiring
image::images/wiringup.png[The 2 Pipelines wired, width=650, align="center", link="images/wiringup.png"]

==== Publish/Subscribe
For *Fan-Out* to subscribers from a unified pipeline, `Stream.process(Processor)`, `Stream.broadcast()`, `Stream.broadcastOn()` and `Stream.broadcastTo()` can be used.

.Sharing an upstream pipeline and wiring up 2 downstream pipelines
[source, java]
----
import static reactor.Environment.*;
import reactor.rx.Streams;
import reactor.rx.Stream;
//...

Stream<String> stream = Streams.just("a","b","c","d","e","f","g","h");

//prepare a shared pipeline
Stream<String> sharedStream = actionChain1.observe(System.out::println).broadcast();

//prepare two unique pipelines
Streams<Long> actionChain1 = sharedStream.map(String::toUpperCase).filter(w -> w.equals("c"));
Streams<Long> actionChain2 = sharedStream.take(5).count();

actionChain1.consume(System.out::println); //start chain1
actionChain2.consume(System.out::println); //start chain2
----

.After Wiring a Shared Stream
image::images/broadcast.png[The 3 Pipelines wired, width=650, align="center", link="images/broadcast.png"]


.Operations considered terminal or explicitely subscribing
[cols="1,1,6", options="header"]
|===

|Stream<T> method
|Return Type
|Role

|subscribe(*Subscriber<T>*), +subscribeOn
|void
|Subscribe the passed *Subscriber<T>* and therefore materialize any pending upstream wired up lazily (the implicit *lift* for non terminal operation). Note a Subscriber must request data if it expects some. The `xxxOn` alternatives provide for `Dispatcher` optional argument to run the subscriber.

|consume(_Consumer<T>,Consumer<T>,Consumer<T>_), _consumeOn_
|Control
|Call `subscribe` with a `ConsumerAction` which interact with each passed optional `Consumer` every time the interest signal is detected. It will `request(Streams.capacity()` to the received `Subscription`, which is Long.MAX_VALUE by default for unbounded consuming.  The `xxxOn` alternatives provide for `Dispatcher` optional argument to run the consumers. Returns a Control argument to eventually cancel the materialized `Stream` if necessary. Note that `ConsumeAction` take care of unbounded recursion if the `onNext(T)` signal triggers a blocking request.

|consumeLater()
|Control
|Similar to `consume` but does not fire an initial `Subscription.request(long)`. The returned `Control` can be used to `request(long)` anytime.

|tap()
|TapAndControls
|Similar to `consume` but return a `TapAndControls` that will be dynamically updated everytime a new `onNext(T)` is signalled and cancelled anytime.


|batchConsume(*Consumer<T>*, _Consumer<T>, Consumer<T>_, *Function<Long,Long>*) _batchConsumeOn_
|Control
|Similar to `consume` but will request the mapped `Long` demand given the previous demand and starting with the default `Stream.capacity()`. Useful for adapting the demand from various factors.

|adaptiveConsume(*Consumer<T>*, _Consumer<T>, Consumer<T>_, *Function<Stream<Long>,Publisher<Long>>*), _adaptiveConsumeOn_
|Control
|Similar to `batchConsume` but will request the computed sequence of demand `Long`. It can be used to insert flow-control such as `Streams.timer()` to delay demand.

|next()
|Promise<T>
|Return a `Promise<T>` that is actively subscribing to the `Stream`, materializing it, and requesting a single data before unregistering. The immediate next signal `onNext(T)`, `onComplete()` or `onError(Throwable)` will fulfill the promise.

|toList()
|Promise<List<T>>
|Similar to `next()` but will wait until all sequence has been produced (`onComplete()`) and pass the accumulated `onNext(T)` in a single `List<T>` fulfilling the returned promise.

|Stream.toBlockingQueue()
|CompletableBlockingQueue<T>
|Subscribe to the `Stream` and return an iterable blocking `Queue<T>` accumulating all `onNext` signals. `CompletableBlockingQueue.isTerminated()` can be used as a condition to exit a blocking `poll()` loop.

|cache()
|Stream<T>
|Turn any Stream into a *Cold* Stream, able to replay all the sequence of signals indivudally for each Subscriber.
Due to the unbounded nature of the action, consider to use with small sequences.

|broadcast(), broadcastOn(Environment, Dispatcher)
|Stream<T>
|Turn Any Stream into a *Hot* Stream. This will prevent pipeline duplication by immediately materializing the `Stream` and be ready to publish the signal to N Subscribers downstream.
The demand will be aggregated from all child Subscribers.

|broadcastTo(*Subscriber<T>*)
|Subscriber<T>
|An alternative to `Stream.subscribe` which allows method chaining since the returned instance is the same than the passed argument.

|process(*Processor<T, O>*)
|Stream<O>
|Similar to broadcast() but accept any given `Processor<T, O>`. A perfect place to introduce <<core-processor.adoc#core-processor, Core Processors>> !


|===

[[stream-capacity]]
=== Setting Capacity

The Reactive Streams standard encourages application developers to set reasonable limits on in-flight data. This prevents components from becoming innudated with more data than they can handle, which causes unpredictable problems throughput an application. One of the core concepts of Reactive Streams is that of "backpressure", or the ability for a pipeline to communicate to upstream components that it can only handle a fixed number of items at a time. A useful term to describe this process of queueing and requesting small chunks of a large volume of data is "microbatching".

Within a Reactor `Stream`, it's possible to microbatch items to limit the amount of data in-flight at any given time. This has distinct advantages in a number of ways, not the least of which is that it limits expsoure to data loss by preventing the system from accepting more data than it can afford to lose if the system was to crash.

To limit the amount of data in-flight in a `Stream`, use the link:/docs/api/reactor/rx/Stream.html#capacity-long-[`.capacity(long)`] method.

.Streams.just()
[source,java]
----
Stream<String> st;

st
  .dispatchOn(sharedDispatcher())
  .capacity(256) // <1>
  .consume(s -> service.doWork(s)); // <2>
----
<1> Limit the amount of data in-flight to no more than 256 elements at a time.
<2> Produce demand upstream by requesting the next 256 elements of data.

[WARNING]
`capacity` will not affect `consume` actions if the current Stream dispatcher set with `dispatchOn` early on is a `SynchronousDispatcher.INSTANCE` (default if unset).

[TIP]
We encourage the *Reactor Users* to study the benefit of setting capacity vs computing dynamic demand with `Stream.adaptiveConsume` or a custom `Subscriber`.

=== Functional Composition

Similar to many other functional libraries, Reactor provides a number of useful methods for composing functions on a `Stream`. You can passively observe values, transform them from one kind to another, filter out values you don't want, buffer values until a size or time trigger is tripped, and many other useful operations.

[IMPORTANT]
These operations are called `Actions`, and they will not <<streams.adoc#wireup,wire up the `Stream` directly>>. They are available on any `Stream` instance, which means <<streams.adoc#streams-basic,you should have one by this stage>>.

****
* `Actions` are `onSubscribe()` in declarative order (left to right), so `stream.actionA().actionB()` will execute actionA first then actionB.
** `onSubscribe()` runs on the parent `Publisher` thread context which can be altered by `subscribeOn(Dispatcher)` for instance.
* `Actions` `subscribe()` in inverse declarative order (right to left). Whenever `subscribe` is excplicitely called at the end of the pipeline, `subscribe()` propagates backward.
** `subscribe()` synchronously propagates back which might affect stack size use. If that becomes an issue, use a delegate `Processor` that runs `subscribe()` on a `Environment.tailRecurse()` dispatcher. Then `process()` it at any point of the chain.
****

==== Observe

If you want to passively observe data as it passes through the pipeline, then use the `.observe(Consumer)` methods and other `reactor.rx.action.passive` actions.
To observe values, use link:/docs/api/reactor/rx/Stream.html#observe-reactor.fn.Consumer-[.observe(Consumer<? super T>)]. To observe errors without dealing with them definitively, use link:/docs/api/reactor/rx/Stream.html#observeError-java.lang.Class-reactor.fn.BiConsumer-[.observe(Class<? extends Throwable>, BiConsumer<Object,? extends Throwable>)]. To observe the Reactive Streams complete signal, use link:/docs/api/reactor/rx/Stream.html#observeComplete-reactor.fn.Consumer-[.observeComplete(Consumer<Void>)]. To observe the cancel signal, use link:/docs/api/reactor/rx/Stream.html#observeCancel-reactor.fn.Consumer-[.observeCancel(Consumer<Void>)]. To observe the Reactive Streams subscribe signal, use link:/docs/api/reactor/rx/Stream.html#observeSubscribe-reactor.fn.Consumer-[.observeSubscribe(Consumer<? super Subscription<T>>)].

.observe(Consumer<T>)
[source,java]
----
Stream<String> st;

st.observe(s -> LOG.info("Got input [{}] on thread [{}}]", s, Thread.currentThread())) // <1>
  .observeComplete(v -> LOG.info("Stream is complete")) // <2>
  .observeError(Throwable.class, (o, t) -> LOG.error("{} caused an error: {}", o, t)) // <3>
  .consume(s -> service.doWork(s)); // <4>
----
<1> Passively observe values passing through without producing demand.
<2> Run once all values have been processed and the `Stream` is marked complete.
<3> Run any time an error is propagated.
<4> Produce demand on the pipeline and consume any values.

==== Filter

It's possible to filter items passing through a `Stream` so that downstream actions only see the data you want them to see. Filtering actions can be found under `reactor.rx.action.filter` package.
The most popular one is the link:/docs/api/reactor/rx/Stream.html#filter-reactor.fn.Predicate-[`.filter(Predicate<T>)`] method.

[NOTE]
Unmatched data will trigger a `Subscription.request(1)` if the stream is actually not unbounded with a previous demand of Long.MAX_VALUE.

.filter(Predicate<T>)
[source,java]
----
Stream<String> st;

st.filter(s -> s.startsWith("Hello")) // <1>
  .consume(s -> service.doWork(s)); // <2>
----
<1> This will only allow values that start with the string `'Hello'` to pass downstream.
<2> Produce demand on the pipeline and consume any values.

==== Limits

A specific application of filters is for setting limits to a `Stream`. Limiting actions can be found under `reactor.rx.action.filter` package.
There are various ways to tell a Stream<T> its boundary in time, in size and/or on a specific condition.
The most popular one is the link:/docs/api/reactor/rx/Stream.html#take-long-[`.take(long)`] method.


.Stream.take(long)
[source,java]
----
Streams
  .range(1, 100)
  .take(50) // <1>
  .consume(
    System.out::println,
    Throwable::printStackTrace,
    avoid -> System.out.println("--complete--")
  );
----
<1> Only take the 50 first elements then cancel upstream and complete downstream.

==== Transformation

If you want to actively transform data as it passes through the pipeline, then use `.map(Function)` and other `reactor.rx.action.transformation` actions.
The most popular transforming action is link:/docs/api/reactor/rx/Stream.html#map-reactor.fn.Function-[.map(Function<? super I, ? extends O>)].
A few other `Actions` depend on transforming data, especially <<streams.adoc#streams-combine,Combinatory operations>> like `flatMap` or `concatMap`.

.Stream.map(Function<T,V>)
[source,java]
----
Streams
  .range(1, 100)
  .map(number -> ""+number) // <1>
  .consume(System.out::println);
----
<1> Transform each Long into a String.

==== (A)Sync Transformation: FlatMap, ConcatMap, SwitchMap

If you want to execute a distinct pipeline `Stream<V>` or `Publisher<V>` given an actual input data, you can use combinatory actions such as `.flatMap(Function)` and other `reactor.rx.action.combination` actions.

To transform values into distinct `Publisher<V>` possibly asynchronous, use link:/docs/api/reactor/rx/Stream.html#map-reactor.fn.Function-[.flatMap(Function<? super I, ? extends Publisher<? extends O>)].
The returned `Publisher<V>` will then be *merged* back to the main flow signaling `onNext(V)`. They are properly removed from the merging action whey they complete.
The difference between flatMap, concatMap and switchOnMap is the *merging strategy*, respectively *Interleave*, *Fully Sequential* and *Partially Sequential* (interrupted by `onNext(Publisher<T>)`).

[IMPORTANT]
The downstream request is splitted (minimum 1 by merged Publisher)

.Stream.flatMap(Function)
[source,java]
----
Streams
  .range(1, 100)
  .flatMap(number -> Streams.range(1, number).subscribeOn(Environment.cachedDispatcher()) ) // <1>
  .consume(
    System.out::println, <2>
    Throwable::printStackTrace,
    avoid -> System.out.println("--complete--")
  );
----
<1> Transform any incoming number into a range of 1-N number merged back and executed on the given Dispatcher.

==== Blocking and Promises

Blocking is considered anti-pattern to *Reactor* but we do offer appropriate API (Ah AH!) for coordination purposes with legacy operations and for test support.

Promise API offers a range of *stateful actions* inspecting first the current state *ready|error|complete* and if fulfilled, immediately calls the invoked action.

.Stream.toList()
[source,java]
----
Promise<List<Long>> result = Streams
  .range(1, 100)
  .subscribeOn(Environment.cachedDispatcher())
  .toList(); // <1>

System.out.println(result.await()); // <2>
result.onSuccess(System.out::println) // <3>
----
<1> Consume all the sequence on the dispatcher thread given in `subscribeOn(Dispatcher)` operation.
<2> Block (default 30 Seconds) until `onComplete()` and print the only `onNext(List<Long>)`, of if `onError(e)` wrap as RuntimeException and raise it.
<3> Since the promise is already fulfilled, `System.out.println()` will run immediately on the current context.

.Waiting for a Stream or Promise
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Streams.await(Publisher<?>)
|Block until the passed Publisher `onComplete()` or `onError(e)`, bubbling up the eventual exception.

|Stream.next() + Promise.await(), Promise.get()...
|Capture in a Promise the immediate next signal only and `onComplete()` if the signal was a data. `get()` can be used to touch but not wait on the promise to fulfill.

|Stream.toList() + Promise.await(), Promise.get()...
|Similar to `next()` but capture the full sequence in a List<T> to fulfill the `Promise<List<T>>` returned.

|Stream.toBlockingQueue()
|Subscribe to the `Stream` and return an iterable blocking `Queue<T>` accumulating all `onNext` signals. `CompletableBlockingQueue.isTerminated()` can be used as a condition to exit a blocking `poll()` loop.

|Wiring up Synchronous Streams
|It's not specific to any API but if the current Stream is dispatched on a `SynchronousDispatcher`, it is actually blocking when a *terminal* action is starting, such as `consume()`.

|===

[[streams-multithreading]]
== Understanding the threading model

So `Stream` are `Publisher` which chain execution with `Action` or `Processor` and terminate with a `Subscriber` or a terminal `Action`.
One common purpose for *Reactive Streams* and *Reactive Extensions* is to be non-opinionated about threading behavior *thanks to the signal callbacks*.
Streams are all about *it will be executed at some point between now and some time T*. Non concurrent signals may also preserve `Subscriber` from concurrency access (share-nothing),
however signals and requests can run on 2 asymmetric threads.

By default the `Stream` is assigned with a `SynchronousDispatcher` and will inform its immediate child `Actions` via `Stream.getDispatcher()`.

[IMPORTANT]
Various `Streams` factories, the `Broadcaster`, the `Stream.dispatchOn`  and the terminal `xxxOn` methods might alter the default `SynchronousDispatcher`.

.It is fundamental to understand the three major thread switchs available in Reactor Stream:
****
* The `Stream.dispatchOn` action is the only one available under `Stream` that will be dispatching *onError*, *onComplete* and *onNext* signals on the given `Dispatcher`. 
** Since an action is a `Processor` it doesn't support concurrent `Dispatcher` such as `WorkQueueDispatcher`.
** `request` and `cancel` will run on the dispatcher as well if in its context already. Otherwise it will execute after the current dispatch ends.
** `Streams.dispatchOn(SynchronousDispatcher)` has no effect
* The `Stream.subscribeOn` action will be executing *onSubscribe* only on the passed dispatcher.
** Since the only time the passed `Dispatcher` is called is *onSubscribe*, any dispatcher can be used including the concurrent ones such as `WorkQueueDispatcher`.
** The first `request` might still execute in the *onSubscribe* thread, for instance with `Stream.consume()` actions.
** `Streams.subscribeOn(SynchronousDispatcher)` has no effect
* Attaching a `Processor` via `Stream.process` for instance can affect the thread too. The `Processor` such as `RingBufferProcessor` will run the `Subscribers` on its managed threads. 
** `request` and `cancel` will run on the processor as well if in its context already.
** `RingBufferWorkProcessor` will only dispatch *onNext* signals to one `Subscriber` at most unless it has cancelled in-flight (replay to a new Subscriber).
****

Since the common contract is to start requesting data *onSubscribe*, `subscribeOn` is an efficient tool to scale-up streams, particulary unbounded ones.
If a `Subscriber` requests *Long.MAX_VALUE* *onSubscribe*, it will then be the only request executed and it will run on the dispatcher assigned in `subscribeOn`.
This is the default behaviour for unbounded `Stream.consume` actions.

.Jumping between threads with an unbounded demand
[source,java]
----
Streams
  .range(1, 100)
  .subscribeOn(Environment.cachedDispatcher()) // <1>
  .dispatchOn(Environment.sharedDispatcher()) // <2>
  .consume(); // <3>
----
<1> Assign an *onSubscribe* cached dispatcher.
<2> Assign a signal *onNext, onError, onComplete* dispatcher.
<3> Consume the `Stream` *onSubscribe* with `Subscription.request(Long.MAX)`

.subscribeOn and dispatchOn/process with an unbounded Subscriber
image::images/longMaxThreading.png[Unbounded threading, width=600, align="center", link="images/longMaxThreading.png"]

However, `subscribeOn` is less useful when more than 1 request will be involved, like in step-consuming with `Stream.capacity(n)`.
The only request executed possibly running on the dispatcher assigned in `subscribeOn` is the *first one*.

.Jumping between thread with a bounded demand 1
[source,java]
----
Streams
  .range(1, 100)
  .subscribeOn(Environment.cachedDispatcher()) // <1>
  .process(RingBufferProcessor.create()) // <2>
  .capacity(1); // <3>
  .consume(); // <4>
----
<1> Assign an *onSubscribe* cached dispatcher.
<2> Assign an async signal *onNext, onError, onComplete* processor. Similar to `dispatchOn` behavior.
<3> Assign a `Stream` capacity to 1 so the downstream action adapts
<4> Consume the `Stream` *onSubscribe* with `Subscription.request(1)` and after every 1 *onNext*.

.subscribeOn and dispatchOn/process with an bounded (demand N < Long.MAX) Subscriber
image::images/nThreading.png[Bounded threading, width=600, align="center", link="images/nThreading.png"]

[[streams-microbatching]]
== MicroBatching

"Better trade your unused CPU and Memory for your overused Latency"
-- Klingon Proverb

After one or two reads of the <<streams.adoc#stream-basics,101 Stream crash intro>>, you courageaous hacker are ready for some _quick ROI_.
In effect dispatching efficiently is far away from the only item to check in the *way of millions messages per sec todo list*.

A common issue in *Distributed Systems* lies into the latency cost over indivudual vs buffered IO writes.
When such situation arises, *MicroBatching* or _small chunk-processing_ is the action to group individual data operations.
Behind the term `Micro` hides a more concrete behavior named *In Memory*. Since the Speed of Light is still having a remarked impact in
Systems as of today, main memory remains cheaper to read than *disk*.

====
Latency Comparison Numbers
--------------------------
L1 cache reference                            0.5 ns
Branch mispredict                             5   ns
L2 cache reference                            7   ns             14x L1 cache
Mutex lock/unlock                            25   ns
Main memory reference                       100   ns             20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy              3,000   ns
Send 1K bytes over 1 Gbps network        10,000   ns    0.01 ms
Read 4K randomly from SSD*              150,000   ns    0.15 ms
Read 1 MB sequentially from memory      250,000   ns    0.25 ms
Round trip within same datacenter       500,000   ns    0.5  ms
Read 1 MB sequentially from SSD*      1,000,000   ns    1    ms  4X memory
Disk seek                            10,000,000   ns   10    ms  20x datacenter roundtrip
Read 1 MB sequentially from disk     20,000,000   ns   20    ms  80x memory, 20X SSD
Send packet CA->Netherlands->CA     150,000,000   ns  150    ms

Notes
-----
1 ns = 10-9 seconds
1 ms = 10-3 seconds
* Assuming ~1GB/sec SSD

Credit
------
By Jeff Dean:               http://research.google.com/people/jeff/
Originally by Peter Norvig: http://norvig.com/21-days.html#answers
====

`Streams` are sequences of data, so finding boundaries to cut aggregated buffers is an out-of-the box  API.

.There are two categories for delimitations:
****
* *Buffer* : Concrete boundaries *accumulating* `onNext(T)` inside grouped `List<T>` passed to the child `Subscriber`.
** Used best with external API requiring `Iterable<T>` input argument.
* *Window* : Discrete boundaries *forwarding* `onNext(T)` into distinct `Stream<T>` passed to the child `Subscriber`.
** Used best with accumulators such as `reduce` or any subscriber/action reacting to `onComplete()`.
** Can be combined with `flatMap` or `concatMap` which merge back the individual windows in a common `Stream<T>`
****

=== Into Buffers

Collecting grouped sequences of data `T` into lists `List<T>` serves two main purposes:

****
* Expose a sequence matching the boundary conditions into an `Iterable` structure commonly used by JVM APIs
* Reduce the volume of `onNext(T)` signals, e.g. `buffer(5)` will transform a sequence of 10 elements into a sequence of 2 lists (of 5 elements).
****

[NOTE]
Collecting data incurs an overhead in memory and possibly CPU that should be sized appropriately. Small and timed boundaries are advised to avoid any long lasting aggregates.

[WARNING]
An `Environment` must be initialized if the timed `buffer()` signatures are used without providing the `Timer` argument.


[source,java]
----
long timeout = 100;
final int batchsize = 4;
int parallelStreams = 16;
CountDownLatch latch = new CountDownLatch(1);

final Broadcaster<Integer> streamBatcher = Broadcaster.<Integer>create(env);
streamBatcher
  .buffer(batchsize, timeout, TimeUnit.MILLISECONDS)
  .consume(i -> latch.countDown()));


streamBatcher.onNext(12);
streamBatcher.onNext(123);
streamBatcher.onNext(42);
streamBatcher.onNext(666);

boolean finished = latch.await(2, TimeUnit.SECONDS);
if (!finished)
  throw new RuntimeException(streamBatcher.debug().toString());
else {
  System.out.println(streamBatcher.debug().toString());
  assertEquals("Must have correct latch number : " + latch.getCount(), latch.getCount(), 0);
}
----

.Chunk processing with Stream buffers (returning Stream<List<T>>):
[cols="2,6", options="header"]
|===
|Stream<T> API
|Role

|buffer(_int_)
|Aggregate until `onComplete()` or the given `int` argument is reached which starts over a new aggregation.

|buffer(*Publisher<?>*, _Supplier<? extends Publisher<?>>_)
|Aggregate until `onComplete()` or when the first `Publisher<?>` argument emits a signal. The optional `Supplier<? extends Publisher<?>>` supplies a sequence whose first signal will end the linked aggregation. That means overlapping (sliding buffers) and disjoint aggregation can be emitted to the child `Subscriber<List<T>>`.

|buffer(*Supplier<? extends Publisher<?>>*)
|Aggregate until `onComplete()` or in coordination with a provided `Publisher<?>`. The `Supplier<? extends Publisher<?>>` supplies a sequence whose first signal will ends the linked aggregation and starts a new one immediately. 

|buffer(*int, int*)
|Aggregate until `onComplete()` or the given second *skip* `int` argument is reached which starts over a new aggregation. The first *size* `int` argument will delimit the maximum numger of aggregated elements by buffer. That means overlapping (sliding buffers) and disjoint aggregation can be emitted to the child `Subscriber<List<T>>`.

|buffer(*long*, TimeUnit, Timer_)
|Aggregate until `onComplete()` or the elpased *period* `long` argument is reached which starts over a new aggregation. 

|buffer(*long, long*, TimeUnit, Timer_)
|Aggregate until `onComplete()` or the given second *timeshift* `long` argument is reached. The first *timespan* `long` argument will delimit the maximum numger of aggregated elements by buffer. That means overlapping (sliding buffers) and disjoint aggregation can be emitted to the child `Subscriber<List<T>>`.

|buffer(*int, long*, _TimeUnit, Timer_)
|A combination of `buffer(int)` *OR* `buffer(long, TimeUnit, Timer)` conditons. It accumulates until the given *size* has been reached or the *timespan* elapsed.

|===

=== Into Windows

Forwarding grouped sequences of data `T` into streams `Stream<T>` serves three main purposes:

****
* Expose a sequence of data `T` to various limited grouped observations and accumulation: metrics, average, flexible aggregate (`Map`, `Tuple`...).
* Parallelizing grouped sequences combined with `dispatchOn` for each generated `Stream<T>` and merging their results back.
* Repeat `onComplete()` for individual grouped sequences, e.g. in <<net.adoc#net-overview, Async IO` module to delimit a flush.
****


[NOTE]
====
`Stream<T>` windows are slightly less optimized but equivalent aggregating producer than buffer API if combined with the aggregate-all `Stream.buffer()` method:

[source,java]
----
stream.buffer(10, 1, TimeUnit.SECONDS);

//equivalent to
stream.window(10, 1, TimeUnit.SECONDS).flatMap( window -> window.buffer() )
----
====

[WARNING]
An `Environment` must be initialized if the alias for timed `buffer()` are used without providing the `Timer` argument. If an `Environment` is found, its default `Timer` will be used.

[source,java]
----
//create a list of 1000 numbers and prepare a Stream to read it
Stream<Integer> sensorDataStream = Streams.from(createTestDataset(1000));

//wait for all windows of 100 to finish
CountDownLatch endLatch = new CountDownLatch(1000 / 100);

Control controls = sensorDataStream
  .window(100)
  .consume(window -> {
    System.out.println("New window starting");
    window
      .reduce(Integer.MAX_VALUE, (acc, next) -> Math.min(acc, next))
      .finallyDo(o -> endLatch.countDown())
      .consume(i -> System.out.println("Minimum " + i));
  });

endLatch.await(10, TimeUnit.SECONDS);
System.out.println(controls.debug());

Assert.assertEquals(0, endLatch.getCount());
----

.Chunk processing with Stream (returning Stream<Stream<T>>):
[cols="2,6", options="header"]
|===
|Stream<T> API
|Role

|window(_int_)
|Forward to a generated `Stream<T>` until `onComplete()` or the given `int` argument is reached which starts over a new `Stream`.

|window(*Publisher<?>*, _Supplier<? extends Publisher<?>>_)
|Forward to a generated `Stream<T>` until `onComplete()` or when the first `Publisher<?>` argument emits a signal. The optional `Supplier<? extends Publisher<?>>` supplies a sequence whose first signal will ends the linked aggregation. That means overlapping (sliding buffers) and disjoint aggregation can be emitted to the child `Subscriber<Stream<T>>`.

|window(*Supplier<? extends Publisher<?>>*)
|Forward to a generated `Stream<T>`  until `onComplete()` or in coordination with a provided `Publisher<?>`. The `Supplier<? extends Publisher<?>>` supplies a sequence whose first signal will end the linked `Stream<T>` and starts a new one immediately. 

|window(*int, int*)
|Forward to a generated `Stream<T>`  until `onComplete()` or the given second *skip* `int` argument is reached which starts over a new `Stream<T>`. The first *size* `int` argument will delimit the maximum numger of aggregated elements by buffer. That means overlapping (sliding buffers) and disjoint sequence can be emitted to the child `Subscriber<Stream<T>>`.

|window(*long*, TimeUnit, Timer_)
|Forward to a generated `Stream<T>`  until `onComplete()` or the elpased *period* `long` argument is reached which starts over a new `Stream<T>`. 

|window(*long, long*, TimeUnit, Timer_)
|Forward to a generated `Stream<T>`  until `onComplete()` or the given second *timeshift* `long` argument is reached. The first *timespan* `long` argument will delimit the maximum numger of aggregated elements by buffer. That means overlapping (sliding buffers) and disjoint sequence can be emitted to the child `Subscriber<Stream<T>>`.

|window(*int, long*, _TimeUnit, Timer_)
|A combination of `buffer(int)` *OR* `buffer(long, TimeUnit, Timer)` conditons. It forwards to a generated `Stream<T>` until the given *size* has been reached or the *timespan* elapsed.

|===

[[streams-backpressure]]
== Backpressure and Overflow

Backpressure is addressed automatically in a bunch of situations with the *Reactive Streams* contract. If a `Subscriber` doesn't request more than it can actually process (request different from `Long.MAX_VALUE`), the upstream source can avoid sending more data. That only works with cold data `Publisher` which can stop-read anytime the mapped source: _How much read from a socket, How much rows from a SQL query, how much lines from a File, how much elements from an Iterable_...

If the source is *hot* such as a timer or UI events or the `Subscriber` might request `Long.MAX_VALUE` on a large dataset, a strategy must be explicitely picked by the developer to confront the *backpressure*. 

*Reactor* provides a set of API to deal with both situations.


.Controlling the volume of in-flight data
[cols="2,6", options="header"]
|===

|Stream<T>
|Role

|subscribe(*Subscriber<T>*)
|A custom `Subscriber<T>` will have all the flexibility to request whenever it wishes. Best to size these requests if the `Subscriber` involve some blocking operations.

|batchConsume(*Consumer<T>*, _Consumer<T>, Consumer<T>_, *Function<Long,Long>*) _batchConsumeOn_
|Control
|Similar to `consume` but will request the mapped `Long` demand given the previous demand and starting with the default `Stream.capacity()`. Useful for adapting the demand from various factors.

|adaptiveConsume(*Consumer<T>*, _Consumer<T>, Consumer<T>_, *Function<Stream<Long>,Publisher<Long>>*), _adaptiveConsumeOn_
|Control
|Similar to `batchConsume` but will request the computed sequence of demand `Long`. It can be used to insert flow-control such as `Streams.timer()` to delay demand.

|onOverflowBuffer(_CompletableQueue_)
|Create or use the given `CompletableQueue` to store the overflow elements. Overflow occurs when a `Publisher` sends more than actually requested for. Overflow will be drained over the next requests.

|onOverflowDrop()
|Ignore the overflow elements. Overflow occurs when a `Publisher` sends more than actually requested for. Overflow will be drained over the next requests.

|_All_ sample(), sampleFirst()
|Reduce the volume of a `Stream<T>` by selecting the last (or the first) `onNext(T)` signal matching the given conditions. These condition can be timed, sized, timed or sized, and interactive (event-driven).

|throttle(long)
|Delay downstream `request(long)` 

|Stream.capacity(long)
|<<streams.adoc#stream-capacity, Set the capacity>> to this `Stream<T>` and all downstream actions.

|Stream.process( RingBuffer*Processor )
|

|Stream.filter(), take(), takeWhile()...
|All limit operations can be used to proactively limit the volume of a Stream.

|Stream.buffer(), reduce(), count()...
|All aggregating and metrics operations can be used to proactively limit the volume of a Stream.

|===

[[streams-combine]]
== Combinatory Operations
TODO Combining multiple Reactive Streams data `Publishers` in an controlled fashion.

.Combining Data Sources
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.flatMap
|

|Streams.switchOnNext(Publisher<Publisher<T>>)
|A Stream alterning in FIFO order between emitted `onNext(Publisher<T>)` from the passed Publisher. The signals will result in downstream Subscriber<T> receiving the next Publisher sequence of `onNext(T)`.
It might interrupt a current upstream emission when the `onNext(Publisher<T>)` signal is received.

|Streams.concat(*Publisher<T>,Publisher<T>?* x7),
Streams.concat(Publisher<Publisher<T>>),
Stream.concatWith(Publisher),
Stream.startWith(Publisher)
|If a Publisher<T> is already emitting, wait for it to `onComplete()` before draining the next pending Publisher<T>.

|Streams.merge, Stream.mergeWith
|

|Streams.combineLatest
|

|Streams.zip, Stream.zipWith,
|

|Streams.join, Stream.joinWith
|

|===

[[streams-microservice]]
== MicroServices

TODO Using a `Stream` or a `Promise` to deal with remote `MicroService` latency.

.Reading remote resources
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.create(Publisher), Stream.defer(Supplier), Stream.wrap(Publisher), Stream.generate(Supplier)
|

|Stream.timeout()
|

|Stream.take()
|

|Stream.flatMap
|

|Stream.subscribeOn(), Stream.dispatchOn()

|===

[[streams-microservice-start]]
=== Creating Non-Blocking Services
[[streams-microservice-compose]]
=== Composing multiple Services Calls
[[streams-microservice-share]]
=== Choosing the right (multi)threading strategy

[[streams-errors]]
== Error Handling
Using a `Stream` to build fault tolerant pipelines.


.Handling errors
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.when(Class<Throwable>, Consumer<Throwable>)
|

|Stream.materialize()
|

|Stream.recover()
|

|Stream.observeError()
|

|Stream.onErrorReturn()
|

|Stream.onErrorResume()
|

|Stream.retry()
|

|Stream.oberveError()
|

|Stream.ignoreErrors()
|

|*throw* CancelException
|

|===

[[streams-persistent]]
== Persisting Stream Data
Combining multiple Reactive Streams data `Publishers` in an controlled fashion.

.Persisting signals safely
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.onOverflowBuffer()
|

|IOStreams.persistentMapReader()
|

|IOStreams.persistentMap()
|

|===

[[streams-analytics]]
== Analytics
Using a `Stream` to compute metrics and any inline stateful processing.

.Operations useful for metrics and other stateful accumulation.
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.count()
|

|Stream.scan()
|

|Stream.reduce()
|

|BiStreams.reduceByKey()
|

|BiStreams.scanByKey()
|

|Stream.timestamp()
|

|Stream.elapsed()
|

|Stream.materialize()
|


|===

[[streams-partition]]
== Partitioning
Partition a `Stream` for concurrent, parallel work.

An important aspect of the functional composition approach to reactive programming is that work can be broken up into discreet chunks and scheduled to run on arbitrary Dispatchers. This means you can easily compose a flow of work that starts with an input value, executes work on another thread, and then passes through subsequent transformation steps once the result is available. This is one of the more common usage patterns with Reactor.


[source,java]
----
List<String> ids = Arrays.asList("1", "2", "3", "4", "5", "6", "7", "8", "9", "10");

DispatcherSupplier supplier1 = Environment.newCachedDispatchers(2, "pool1");
DispatcherSupplier supplier2 = Environment.newCachedDispatchers(5, "pool2");

CountDownLatch latch = new CountDownLatch(10);

Streams.from(ids)
  .dispatchOn(Environment.sharedDispatcher())
  .partition(2)
  .flatMap(stream -> stream
    .dispatchOn(supplier1.get())
    .map(s -> s + " " + Thread.currentThread().toString())
  )
  .map(t -> {
    System.out.println("First partition: "+Thread.currentThread() + ", worker=" + t);
    return t;
  })
  .partition(5)
  .flatMap(stream -> stream
    .dispatchOn(supplier2.get())
    .map(s -> s + " " + Thread.currentThread().toString())
  )
  .dispatchOn(Environment.sharedDispatcher())
  .consume(t -> {
    System.out.println("Second partition: "+Thread.currentThread() + ", worker=" + t);
    latch.countDown();
  });


assertThat("Not totally dispatched", latch.await(30, TimeUnit.SECONDS));
----

.Grouping operations
[cols="2,6", options="header"]
|===

|Functional API or Factory method
|Role

|Stream.groupBy()
|

|Stream.partition()
|

|Stream.process(XXXWorkProcessor)
|

|===


[[streams-notrx]]
== Other API beyond Rx

In addition to implementing directly the Reactive Streams, some more `Stream` methods not covered differ or are simply not documented by Reactive Extensions.

.Other methods uncovered in the previous use cases.
[cols="2,6", options="header"]
|===

|Stream method
|Role

|after
|

|log
|

|split
|

|sort
|

|requestWhen
|

|combine
|

|keepAlive
|

|Action.debug(), StreamsUtils.debug(Stream)
|A quick
|===